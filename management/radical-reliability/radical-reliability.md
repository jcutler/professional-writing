# Practicing Radical Reliability as a Leader

**Standing Upright in a Messy System**

Most management writing assumes a reasonably functional system: aligned incentives, honest communication, and enough stability that trust can be built gradually and safely.

Many of us don't work in those systems.

We work in organizations that are opaque and anxious, shaped by periodic layoffs, reorganizations, shifting priorities, and optimization pressures that quietly reward appearance over substance. In those environments, the question isn't how to be inspirational...

**It's how to be _reliable._**

Radical Reliability is not a framework for fixing broken systems. It's a way of deciding how to behave inside them, especially when they're under strain.

It grew out of repeated, ordinary decisions:
- what to say when the truth is uncomfortable,
- how to act when failure is visible,
- how to distribute judgment instead of hoarding it,
- and how to model the kind of work you want others to feel safe doing.

**Much of this approach comes from systems thinking — not applied to software, but to people.**

In reliable systems, we don't assume components are perfect. We assume they will fail, and we design for detection, repair, and resilience. Radical Reliability applies the same logic to human systems: failure is expected, repair is visible, and responsibility lives as close to the work as possible.

## A. Truth-Telling With Consequences, Not Comfort

### The problem

During periods of uncertainty — reorganizations, strategy shifts, cost pressure — managers are often encouraged to project stability even when none exists.

The intent is usually kind: protect morale, keep people focused, prevent panic.

The effect is often the opposite.

When people sense risk but are told not to worry, they stop trusting both leadership _and their own perception._ The gap between lived experience and official messaging creates unpredictable uncertainty — the most exhausting and corrosive kind.

### How I approach it

When I can see risk, I name it.
When the future is unclear, I say so plainly.

I don't promise outcomes I can't control, and I don't relay optimistic narratives without also explaining constraints. I'm careful not to dramatize or speculate — but I refuse to anesthetize reality.

Truth-telling isn't a one-time disclosure. I stay present with the consequences: answering questions, sitting with discomfort, and accepting that clarity sometimes makes people uneasy.

What I refuse to do is trade long-term trust for short-term comfort.

### How this changes behavior

Over time, this shifts how people orient themselves toward uncertainty:

When people see uncomfortable truths named early — not punished, delayed, or euphemized — they learn that honesty is survivable.

They stop spending energy decoding subtext and start spending it on planning, learning, and problem-solving.*

Just as importantly, they learn that they are allowed to surface issues before those issues are polished, defensible, or complete. The cost of speaking early drops.

### What I'm aiming for

I'm trying to replace false reassurance with shared understanding.

Known risk is something adults can reason about together. It allows people to decide how much energy to invest, when to push, and when to protect themselves.

### The takeaway

> **Reliability begins with legibility.**

People don't need managers to make the world safer than it is.
They need managers who won't distort reality to manage feelings.

Truth-telling is necessary, but it is not sufficient. When the truth doesn’t fix the problem, responsibility doesn’t end — it deepens.

## B. Predictability Over Reassurance

### The problem

Many leaders try to build trust by appearing consistently confident and composed.

They avoid showing mistakes, uncertainty, or unfinished thinking, believing that calm authority makes people feel safe.

What this often creates instead is opacity. Leaders who look perfect are unknowable under stress. When something finally breaks — and it always does — people don't know what will happen next.

At that point, people stop paying attention to what went wrong and start paying attention to how leaders respond.

This is when where repair begins. Here, “repair” includes fixing what broke, and goes on to restore trust, clarity, and capacity after harm, so people don’t have to harden themselves just to keep working.

### How I approach it

I let people see me fail.

I narrate my decisions, including the ones that didn't land. I talk about the work that I took on that I shouldn't have, and tradeoffs I misjudged. I share thinking while it's still forming, not just after it's been cleaned up.

When something breaks, I focus on repairing it visibly rather than quietly smoothing it over.

This isn't vulnerability as confession.

It's transparency about *process*.

### How this changes behavior

When people see a manager share unfinished work, admit mistakes, and repair without defensiveness, they learn something essential:

> **Failure doesn't exile you from competence.**

That lesson changes behavior. People stop waiting for perfection before sharing their own work. Iteration speeds up. Problems surface earlier.

People fail faster — and more safely — because they've seen that failure followed by repair is the norm.

### What I'm aiming for

I want people to be able to reason about me under stress.

When things go wrong, they shouldn't wonder whether I'll disappear, overreact, or assign blame. They should already know.

### The takeaway

_Trust isn't built by the absence of failure.
It's built by the **predictability of repair.**_

## C. Treating Engineers as Moral and Economic Agents _(or, the Decentralization of Dignity)_

### The problem

Engineers are often expected to absorb urgency and responsibility without being trusted with real judgment.

On-call work, incident response, operational cleanup demand heroics — but deny people the authority to decide when effort is no longer worth the cost.

Pivots in planning — when a project that was once urgent suddenly isn't — erode trust if the reasoning behind that change isn't communicated. What feels like arbitrariness becomes a demand for blind acceptance.

These mismatches breed burnout and quiet resentment.

### How I approach it

I explicitly teach engineers to reason about tradeoffs.

When on call, that often means a simple calculation: _the financial impact of a problem versus the cost of the human time required to fix it._

I make it clear that it is sometimes correct to let the company lose money — especially when the alternative is burning hours of human energy or eroding customer trust.

I encourage quick, customer-first repairs even when they cost the business something.

And crucially: I back people when they make those calls.

### How this changes behavior

When people are trusted to make economic and ethical decisions, they stop acting like resources and start behaving like individual **stewards of resources.**

They conserve energy. They focus on what matters. They think in terms of long-term impact rather than short-term appeasement.

Escalation drops. Permission-seeking drops. Responsibility rises.

**What emerges from this approach is something quietly prosocial[^1].**

When people are given clear information, real authority, and visible repair, they begin to make decisions that reduce harm and improve outcomes for others without requiring coordination, enforcement, or reward.

This isn't because they've become more altruistic.
It's because the system no longer penalizes them for acting responsibly.

> **People do the right thing when the right thing stops being the most expensive option.**

Cooperation becomes the lowest-friction path, not the riskiest one.

### What I'm aiming for

I want engineers to feel ownership not just of systems, but of judgment — including judgment about when not to fix something.

### The takeaway

> **Dignity scales when decision-making does.**

If you don't trust people with tradeoffs, you don't actually trust them.

## D. Value Installation via Lived Heuristics, Not Principles

### The problem

Most organizational values are declarative. They sound reasonable but offer little guidance when decisions are ambiguous, pressured, or costly.

Under stress, people default to incentives, not slogans.

### How I approach it

Instead of emphasizing abstract values, I teach and repeat simple heuristics — questions people can use when I'm not around:
- Is this problem worth your time?
- Can we see the messy version now instead of the perfect version later?
- Who benefits from this repair?
- What happens if we don't fix this?

I make my own reasoning visible so people can copy the logic, not just the outcome.

### How this changes behavior

Over time, people begin making these decisions without prompting.

They surface work earlier.
They iterate more openly.
They correct faster.

Culture becomes portable.

These heuristics bias people toward prosocial decisions without requiring coordination. Over time, they reduce harm and improve collective outcomes — even when no rule applies and no one is watching.

### The takeaway

**Values are only real if they function in your absence.**

## Misuse and Failure Modes

_Radical Reliability is intentionally anti-heroic._
It assumes that if a system requires constant sacrifice to function, the system — not the people — is at fault.

That being said, Radical Reliability can fail if applied carelessly:
* Honesty without responsibility becomes cruelty.
* Vulnerability without repair creates anxiety.
* Decentralization without support feels like abandonment.

“Just being realistic” can become an excuse for laziness or lack of care.

Overuse without boundaries will burn managers out.

Reliability includes knowing when to rest, when to delegate, and when to stop carrying more than is sustainable, and making those boundaries explicit rather than silently dropping work.

## Moral Influence: Responsibility Without Reward

This approach didn't come from management books. Honestly, it came from stories. Specifically from two of my favorite characters in Terry Pratchett's Discworld series: Sam Vimes and Tiffany Aching.

What always stood out to me about them wasn't that they were heroic or idealistic.

It was that, given the information they had, **doing the wrong thing was harder to imagine than the right one.**

They don't believe people act responsibly because they are rewarded, praised, or recognized as "good." They believe something quieter and more demanding:

_People act responsibly when they are given the right information, the right tools, and the clear expectation of responsibility._

### Sam Vimes: responsibility to the world as it is

Sam Vimes is the Commander of the City Watch of Ankh-Morpork, a notoriously rich and unmanageable city. His approach to leading officers of the Watch centered around being explicit about where responsibility flows.

Members of the Watch are not responsible to _him._
They are responsible to the city.

His orders matter only insofar as they serve the people who live there. That's why his officers are expected to challenge him, verify his assumptions, and disobey if necessary.

Obedience is not the goal.
Alignment is.

What makes this work isn't authority. It's predictability of intent. The Watch trusts that Vimes' orders will be in the best interests of the city as it actually is — not the city as powerful people wish it were.

That trust isn't blind. It's earned through correction and repair.

### Tiffany Aching: responsibility without fault

Tiffany Aching is a young witch from a long line of hard-working shepherds on the Chalk, a place with a long history of burning witches. She operates from an even starker premise than Sam.

She doesn't believe the world will become fair if everyone follows the rules. For her, responsibility has nowhere else to land — it arises from proximity, not blame.

In _A Hat Full of Sky,_ one of the characters puts it this way:

> "There isn't a way things should be.
> There's just what happens, and what we do."

Tiffany takes responsibility not because the problem is hers, but because she is the one who sees it and has the **capacity to act.**

Responsibility isn't about fault.
**It's about being the person who is there.**

### How this shapes Radical Reliability

These worldviews map directly onto how I manage.

I don't treat engineers as people who need incentives to behave responsibly. I treat them as people who already want to act responsibly — provided they are trusted, informed, and expected to do so.

Responsibility isn't something I grant as a reward.
It's something I assume.
And then support.

Authority, in this model, is not control.
It's alignment, combined with the expectation that you can — _and should_ — be challenged when you drift.

That belief is the moral backbone of Radical Reliability.

## Final Thought

Radical Reliability isn't about fixing broken systems.

It's about refusing to let those systems decide who you are while you work inside them.

It's a commitment to legibility, repair, dignity, and shared responsibility — not because they are fashionable, but because they reliably reduce harm and help people do good work together.

It's a way of standing upright in a messy system and being someone others can rely on when things inevitably go wrong.
